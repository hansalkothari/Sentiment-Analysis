# -*- coding: utf-8 -*-
"""Sentiment_Anlysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ITYjkJhRG_Cu0GrVyJj0dqy6T6ebQ0jF
"""

print("jai shree ram!!")

!pip3 install numpy

!pip3 install keras

import numpy
from numpy import array
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM, Dropout
from keras.layers import Embedding
from keras.preprocessing import sequence
from keras.models import load_model
import re
import numpy as np
from nltk.tokenize import word_tokenize
import nltk

# fix random seed for reproducibility
numpy.random.seed(7)

# Load the dataset but only keep the top n words and zero out the rest i.e keep vocabulary size as 5000
top_words = 5000 #vocabulary_size = 5000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)

data2 = imdb.load_data(num_words = top_words)
print(data2)

'''Inspect a sample review and its label.Note that the review is stored as a sequence of integers. These are word IDs that 
have been pre-assigned to individual words, and the label is an integer (0 for negative, 1 for positive).'''
print('---review---')
print(X_train[6])
print('---label---')
print(y_train[6])

'''We can use the dictionary returned by imdb.get_word_index() to map the review back to the original words.'''
word2id = imdb.get_word_index()
id2word = {i: word for word, i in word2id.items()}
print('---review with words---')
print([id2word.get(i, ' ') for i in X_train[6]])
print('---label---')
print(y_train[6])

print(word2id)

#Maximum review length and minimum review length.
print('Maximum review length: {}'.format(
len(max((X_train + X_test), key=len))))

print('Minimum review length: {}'.format(
len(min((X_train + X_test), key=len))))

'''In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to maximum words by truncating 
longer reviews and padding shorter reviews with a null value (0). We can accomplish this task using the pad_sequences() function in Keras. Here, setting max_review_length 
to 500.'''

from keras.utils.data_utils import pad_sequences

max_review_length = 500
X_train = pad_sequences(X_train, maxlen=max_review_length)
X_test = pad_sequences(X_test, maxlen=max_review_length)

'''Remember that our input is a sequence of words (technically, integer word IDs) of maximum length = max_review_length, and our output is a binary sentiment 
label (0 or 1).
'''
# create the model
embedding_vecor_length = 32
model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))
model.add(Dropout(0.2))
model.add(LSTM(100))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

'''We first need to compile our model by specifying the loss function and optimizer we want to use while training, as well as any evaluation metrics 
we’d like to measure. Specify the appropriate parameters, including at least one metric ‘accuracy’.'''
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64) RNNs have a memory that allows them to capture dependencies over time.

!apt-get install -y -qq software-properties-common

!apt-get update -qq 2>&1 > /dev/null

!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null

!apt-get -y install -qq google-drive-ocamlfuse fuse

import os

!ls

os.mkdir("/drive/Sentiment_Analysis")

os.chdir("/drive/")

!ls

import sys

sys.path.append('/drive/Sentiment_Analysis')

#Now save the model in required directory
model.save('/drive/Sentiment_Analysis/sentiment_analysis_model_new.h5')

os.chdir("/drive/Sentiment_Analysis")
!ls

model = load_model('/drive/Sentiment_Analysis/sentiment_analysis_model_new.h5')
print("Model Loaded")

from google.colab import files

# Download the saved model file to your local machine
files.download('sentiment_analysis_model_new.h5')

